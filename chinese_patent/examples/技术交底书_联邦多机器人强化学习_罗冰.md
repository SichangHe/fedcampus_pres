# 技术交底书_联邦多机器人强化学习_罗冰

## 1、详细介绍技术背景，并描述已有的与本发明相关的现有技术，说明现有技术的缺点

<!-- 撰写指导：
只写与本发明有关的技术背景以及现有技术、现有技术的缺点。
 -->

面对环境复杂、天气恶劣、地图未知的任务环境,利用智能机器人来代替人前往高危区域完成如侦察探索、应急搜救等工作是目前机器人领域发展的重要方向。其中，机器人的自主导航技术是机器人能否顺利完成复杂任务的基础。相较于传统的控制方法和策略，强化学习近年来的技术发展为提高机器人自主导航能力提供了新的解决方案。然而现存专利如CN111487864A的单机器人强化学习需要采集大量经验样本用于训练，在单机器人下耗时耗力，效率极低。虽然现有专利，如CN113642243A，利用多机器人部署来提高模型的训练效率，但传统的多机器人强化学习方法需要实时传递自身状态和传感器等原始数据，在带宽受限和机器人本地数据隐私保护等场景下难以实施。
联邦学习技术能够在保障原始数据不出本地的前提下实现多各参与方的协作学习，在模型训练的过程中只传输模型参数，从而在通信开销的基础上有效提高了模型的训练效率。基于联邦学习的强化学习在多智能体协作，特别是自动驾驶等领域有着广泛应用前景。然而，现有基于联邦强化学习的专利，如专利CN112348201A，通常采用基于传统监督学习的同步算法，即每个机器人同时间内训练步数后进行模型聚合。该方法没有考虑现实场景中的异构特性，如多机器人在移动速度和模型计算能力上的差异性，导致每个机器人在训练相同步数的执行时间差异较大，进而影响模型的训练效率。

## 2、本发明摘要：针对现有技术存在的技术问题，提出本发明的基本方案或基本思路，以及本发明针对现有技术所具备的整体技术效果

<!-- 撰写指导：
不必描述实施细节，概括说明本发明的方案或思路，并说明由这种方案能带来的技术效果。
 -->

本发明公开了一种面向多机器人协作导航的异步联邦强化学习模型聚合方法。首先，设置异构的训练环境，即，每个机器人拥有不同的，独立的训练环境。其次，各个机器人在自己的训练环境中采集经验样本，基于强化学习算法进行本地模型训练更新。再次，联邦服务器会在给定模型聚合时间下将各个机器人的本地模型进行收集，并按照各个机器人的不同本地迭代次数进行加权聚合，得到全局的联邦模型。然后，联邦服务器将聚合模型分发给各个机器人，并通知各个机器人在该聚合的模型基础上继续本地训练。最后，各个机器人加载联邦模型并继续新一轮的本地训练。
通过多机器人联邦强化学习，各个机器人在不分享本地数据的前提下获得了其他机器人的训练知识，优化了自身的模型，避免了陷入局部最优。在保证了各方本地数据的隐私的同时降低了通讯的负担，最后训练得到更加鲁棒的模型。所提出的异步算法利用不同机器人在不同的训练环境和自身移动速度和模型处理能力的异构性，能够有效降低联邦学习过程的时间成本，提高了全局模型的收敛速度。

## 3、本发明技术方案的详细阐述，应该结合机械图、流程图、原理框图、电路图、时序图进行说明
<!-- 撰写指导：
3．1、充分说明本发明每一个详细技术方案，
3．2、本发明技术方案带来的效果的详细描述，不仅要说明本发明整体能带来的效果，而且由于某些技术特征可以直接导致的技术效果，也应该在这里结合技术特征的说明来描述效果；
3．3、充分说明所有可以实现本发明的实施方式，主要从实现本发明的细节角度考虑是否有相类似的方式方法或者其他变形的方法，如果有，则请列出。
 -->

本发明的主要目的是提供一种面向多机器人协作导航的异步联邦强化学习模型聚合方法。其主要特点是每个机器人并行执行本地环境的模型训练，仅与联邦服务器进行通信并传递模型参数而非本地原始数据。联邦服务器根据不同机器人在异构环境和自身异构属性下的本地模型更新迭代步数进行模型加权聚合，从而提高全局模型的鲁棒性和训练效率。为实现上述目的，本发明的技术方法如下：
（1）为每个机器人设计不同的导航任务以体现环境系统的异构。每个机器人的训练环境相互独立，互不干扰。
 每个机器人位于各自的训练区域中移动采集数据并进行训练，机器人需要采集的数据包括但不限于机器人自身的位置、目标点的位置（位置信息由gps确定）、机器人的声纳或雷达数据，以及视觉图像数据等。
 训练中机器人会采集训练样本（s,a,r,s’，done）并放入replay buffer，其中s为机器人当前状态，a为机器人基于策略（ε贪心策略）决定的执行动作，r是执行完动作后机器人获得的奖励值，s'为机器人执行完动作后的下一个状态，done为是否结束当前周期的训练并重制机器人。机器人会先随机移动采取数据，到达一定训练步数时开始使用采集的数据进行训练。ε贪心策略的随机值ε会随着训练进行逐渐减小。
 机器人开始训练时将会从replay buffer中调取部分数据进行批量梯度下降优化。ddpg算法分为两个网络，actor网络和critic网络。其中，actor网络决定了个体的动作，输出行为a，critic网络用于评价当前的动作，输出Q(s,a)。为了更有效训练网络，引入了target 网络。
（4a）初始化每个机器人的深度Q，μ，Q’,μ’网络参数，其中Q’,μ’为相应的target网络以及模型训练超参数，包括学习率η，折扣因子γ，贪心策略因子ε，迭代总次数T，当前迭代次数t，
（4b）根据ε贪心策略选择机器人下一步动作，ε会随着训练进行而减小；
（4c）根据机器人与目标点的距离信息设置奖励，如果到达目标点给予大的正奖励；与障碍物过近或者摔倒给予大的负奖励；其它状态基于其与目标点的距离给予小的负奖励（越近负奖励越少）r数学表达式为r=-e^γd（此处设置较为灵活）。其中，γ为常数参数，d为机器人与目标点之间的距离。
（4d）基于得到的一定步数采集数据后，机器人采取批量梯度下降的优化策略进行本地模型训练，用策略梯度下降更新actor网络的参数：∆θ_μ=1/N∑∇*a Q(s,μ(s))∇*(θ_μ ) μ(s);优化损失函数Loss=1/N∑〖(Q(s_t,a_t)-(r_t+Q’(s_(t+1),μ’(s_(t+1)))))〗^2更新Q,利用较小的梯度更新target网络：θ_Q'=τθ_Q+(1-τ)θ_(Q’) 和 θ_μ'=τθ_μ+(1-τ)θ_(μ’)。每训练一定步数机器人会保存其模型。
（5）在训练一定时间T后，联邦服务器开始聚合所有机器人的本地模型，即机器人actor和critic网络参数。
（5a）每间隔一定训练时间T，联邦服务器会命令所有机器人完成当前动作并停止等待联邦服务器聚合模型。
（5b）联邦服务器接收所有机器人最新的保存训练模型，进行聚合。由于环境的异构性以及不同机器人的执行速度和处理能力的异构性，在单位时间内各个机器人训练的步数会有所不同，所提出的异步模型聚合算法如下：θ_f=(v_1 θ_(1 )+v_2 θ_(2 )+...+v_n θ_(n ))/(v_1+v_2+...+v_n )。其中，θ_f为联邦模型的参数，θ_n为各个机器人的模型参数，v_n为机器人训练的总步数。
（5c）聚合完成后，联邦服务器将聚合的模型发分给所有机器人，并通知机器人可以继续训练。机器人收到模型后立即加载聚合模型并开启新一轮的本地训练，并随时等待联邦服务器下一次聚合。

## 4、本发明创新的关键点和想保护的技术方案是什么？

<!-- 撰写指导：
与已知的现有技术相比，本发明会有一些与其不同的关键区别点，这些关键区别点通常会带来明显的或重要的技术效果，这里请说明哪些点是关键区别点，并将这些点可能的组合方案都列举出来。 -->

本发明与现有方法相比具有如下优点：
第一，与基于强化学习的单机器人导航任务策略相比，本发明通过在线多机器人联邦学习的训练方法，让每个机器人能够同时学到其它机器人的模型知识，得到的模型泛化能力相比单个机器人更强，极大地增加了整个系统的可拓展性和鲁棒性。
第二，在训练过程中机器人只需要与联邦服务器进行模型传递，而不是本地原始感知和控制数据，特别是在高精度导航等实时性要求高和海量感知数据的场景中，极大的降低了通信成本开销，提高了训练效率。
第三，本发明提出的多机器人联邦学习异步模型聚合算法针对训练环境的异质性和不同机器人存在的移动速度和处理能力的差异性，提出了按照不同机器人训练步数的异步加权模型聚合算法，与传统同步算法提高了模型的训练效率和时间成本。

## 5、本发明可能的变更设计方向或者变形方案？
<!-- 撰写指导：
从整体考虑，是否有整体的替代或变形方案，或者对核心技术特征的替代或变形方案；主要根据本发明体现出来的设计思路，别人能不能想到其他的整体的替代方案来实现；
从另一个角度讲：考虑授权后，站在竞争对手的角度可能提出的回避设计方案。
 -->

（1）设计更加复杂的训练环境的定义，增加输入维度（如采用多模态感知数据），以及改变奖励函数。
（2）换用其它的深度强化学习算法，但采用相同的联邦算法框架。
（3）换用其他参数进行模型加权聚合，如训练平均奖励。

## 6、发明人补充的其他观点和思路
<!-- 撰写指导：
除1－5部分还需要补充的部分。
 -->

## 7、图及说明
<!-- 撰写指导：
1、交底书部分中未提及的附图标记不得在附图中出现，附图中未出现的附图标记不得在交底书文字中提及；
2、附图中除必须词语（如电路或程序的方框图、流程图、波形图等）外，尽量不要包含有其它文字注释；
3、同一部件或部分的附图标记在前后几幅附图中应一致，同一附图标记不得表示不同的部件或部分；
4、附图集中放在交底书文字之后；
5、附图中，零部件的编号可以用数字表示，也可以直接用零部件的名称来表示。
 -->

图1为本发明摘要流程图。
图2为单机器人强化学习流程示意图。
图3为本发明异步联邦强化学习的多机器人导航控制示意图。
图4为示意图。
